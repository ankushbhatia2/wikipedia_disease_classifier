{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Disease classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook to train a binary classifier to identiy whether a given wikipedia article describes a disease or not.\n",
    "\n",
    "I have used Latent Dirichlet Allocation (LDA) to generate latent distribution of topics for the given dataset. Each document of the dataset is converted into a distribution of topics and each topic itself is a distribution of words in that topic. \n",
    "I used the topic distribution vectos as my features and trained a fully connected neural network with 3 hidden layers.\n",
    "\n",
    "And, to extract the attributes related to the disease I used the infobox of the wikipedia page to get as much information available.\n",
    "\n",
    "Here's a sample infobox of wikipedia article about syphilis :\n",
    "\n",
    "<img src=\"syphilis_info.png\">\n",
    "\n",
    "\n",
    "I used BeautifulSoup to extract the details from the HTML page.\n",
    "\n",
    "Required Packages :\n",
    "```\n",
    "nltk\n",
    "tensorflow==1.15.0\n",
    "gensim\n",
    "stop_words\n",
    "bs4\n",
    "wikipedia\n",
    "numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inports\n",
    "import os, re\n",
    "import wikipedia\n",
    "import stop_words\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec, LdaModel\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = False #Used it for creating a Word2vec model. Not required for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ref = re.compile(\"\\[[0-9]\\]\")\n",
    "def cleanText(text):\n",
    "    return remove_ref.sub(\"\", text).lower()\n",
    "\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stopWords = stop_words.get_stop_words('english')\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stopWords]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_folder):\n",
    "    texts = []\n",
    "    ind = 0\n",
    "    for i in os.listdir(root_folder):\n",
    "        if ind%100 == 0:\n",
    "            print(\"Files loaded\", ind)\n",
    "        ind+=1\n",
    "        with open(os.path.join(root_folder, i), encoding=\"utf8\") as f:\n",
    "            soup = BeautifulSoup(f.read())\n",
    "            content = \"\"\n",
    "            for para in soup.find_all('p'):\n",
    "                content += \" \"+para.text\n",
    "            texts.append(cleanText(content))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded 0\n",
      "Files loaded 100\n",
      "Files loaded 200\n",
      "Files loaded 300\n",
      "Files loaded 400\n",
      "Files loaded 500\n",
      "Files loaded 600\n",
      "Files loaded 700\n",
      "Files loaded 800\n",
      "Files loaded 900\n",
      "Files loaded 1000\n",
      "Files loaded 1100\n",
      "Files loaded 1200\n",
      "Files loaded 1300\n",
      "Files loaded 1400\n",
      "Files loaded 1500\n",
      "Files loaded 1600\n",
      "Files loaded 1700\n",
      "Files loaded 1800\n",
      "Files loaded 1900\n",
      "Files loaded 2000\n",
      "Files loaded 2100\n",
      "Files loaded 2200\n",
      "Files loaded 2300\n",
      "Files loaded 2400\n",
      "Files loaded 2500\n",
      "Files loaded 2600\n",
      "Files loaded 2700\n",
      "Files loaded 2800\n",
      "Files loaded 2900\n",
      "Files loaded 3000\n",
      "Files loaded 3100\n",
      "Files loaded 3200\n",
      "Files loaded 3300\n",
      "Files loaded 3400\n",
      "Files loaded 3500\n",
      "Files loaded 3600\n",
      "Files loaded 0\n",
      "Files loaded 100\n",
      "Files loaded 200\n",
      "Files loaded 300\n",
      "Files loaded 400\n",
      "Files loaded 500\n",
      "Files loaded 600\n",
      "Files loaded 700\n",
      "Files loaded 800\n",
      "Files loaded 900\n",
      "Files loaded 1000\n",
      "Files loaded 1100\n",
      "Files loaded 1200\n",
      "Files loaded 1300\n",
      "Files loaded 1400\n",
      "Files loaded 1500\n",
      "Files loaded 1600\n",
      "Files loaded 1700\n",
      "Files loaded 1800\n",
      "Files loaded 1900\n",
      "Files loaded 2000\n",
      "Files loaded 2100\n",
      "Files loaded 2200\n",
      "Files loaded 2300\n",
      "Files loaded 2400\n",
      "Files loaded 2500\n",
      "Files loaded 2600\n",
      "Files loaded 2700\n",
      "Files loaded 2800\n",
      "Files loaded 2900\n",
      "Files loaded 3000\n",
      "Files loaded 3100\n",
      "Files loaded 3200\n",
      "Files loaded 3300\n",
      "Files loaded 3400\n",
      "Files loaded 3500\n",
      "Files loaded 3600\n",
      "Files loaded 3700\n",
      "Files loaded 3800\n",
      "Files loaded 3900\n",
      "Files loaded 4000\n",
      "Files loaded 4100\n",
      "Files loaded 4200\n",
      "Files loaded 4300\n",
      "Files loaded 4400\n",
      "Files loaded 4500\n",
      "Files loaded 4600\n",
      "Files loaded 4700\n",
      "Files loaded 4800\n",
      "Files loaded 4900\n",
      "Files loaded 5000\n",
      "Files loaded 5100\n",
      "Files loaded 5200\n",
      "Files loaded 5300\n",
      "Files loaded 5400\n",
      "Files loaded 5500\n",
      "Files loaded 5600\n",
      "Files loaded 5700\n",
      "Files loaded 5800\n",
      "Files loaded 5900\n",
      "Files loaded 6000\n",
      "Files loaded 6100\n",
      "Files loaded 6200\n",
      "Files loaded 6300\n",
      "Files loaded 6400\n",
      "Files loaded 6500\n",
      "Files loaded 6600\n",
      "Files loaded 6700\n",
      "Files loaded 6800\n",
      "Files loaded 6900\n",
      "Files loaded 7000\n",
      "Files loaded 7100\n",
      "Files loaded 7200\n",
      "Files loaded 7300\n",
      "Files loaded 7400\n",
      "Files loaded 7500\n",
      "Files loaded 7600\n",
      "Files loaded 7700\n",
      "Files loaded 7800\n",
      "Files loaded 7900\n",
      "Files loaded 8000\n",
      "Files loaded 8100\n",
      "Files loaded 8200\n",
      "Files loaded 8300\n",
      "Files loaded 8400\n",
      "Files loaded 8500\n",
      "Files loaded 8600\n",
      "Files loaded 8700\n",
      "Files loaded 8800\n",
      "Files loaded 8900\n",
      "Files loaded 9000\n",
      "Files loaded 9100\n",
      "Files loaded 9200\n",
      "Files loaded 9300\n",
      "Files loaded 9400\n",
      "Files loaded 9500\n",
      "Files loaded 9600\n",
      "Files loaded 9700\n",
      "Files loaded 9800\n",
      "Files loaded 9900\n"
     ]
    }
   ],
   "source": [
    "pos_texts = get_data(os.path.join(\"training\", \"positive\"))\n",
    "neg_texts = get_data(os.path.join(\"training\", \"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in pos_texts+neg_texts:\n",
    "    tokens.append(apply_all(text))\n",
    "\n",
    "words = [word for sent in tokens for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = FreqDist(words)\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tegucigalpadanl', 1),\n",
       " ('nacaom', 1),\n",
       " ('paraiso', 1),\n",
       " ('guaimaca', 1),\n",
       " ('yoro', 1),\n",
       " ('anillo', 1),\n",
       " ('perifrico', 1),\n",
       " ('expresswaysequip', 1),\n",
       " ('underpassesallow', 1),\n",
       " ('blvdwhich', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 100000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_words,_ = zip(*fdist.most_common(k))\n",
    "top_k_words = set(top_k_words)\n",
    "def keep_top_k_words(text):\n",
    "    return [word for word in text if word in top_k_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [keep_top_k_words(text) for text in tokens] #KEEPING ONLY TOP K WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING LDA MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_lda(data):\n",
    "    num_topics = 300\n",
    "    chunksize = 300\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on \", len(data), \"articles: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model on  13695 articles:  20.611921322345733 min\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus, lda = train_lda(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for ind in range(len(tokens)):\n",
    "    if ind < len(pos_texts):\n",
    "        y.append(1.)\n",
    "    else:\n",
    "        y.append(0.)\n",
    "    bow = dictionary.doc2bow(tokens[ind])\n",
    "    doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n",
    "    X.append(doc_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ankubhat\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\ankubhat\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "dl_model = Sequential()\n",
    "activation = \"relu\"\n",
    "dl_model.add(Dense(128, input_shape=(300,), activation=activation))\n",
    "dl_model.add(Dense(64, activation=activation))\n",
    "dl_model.add(Dense(64, activation=activation))\n",
    "dl_model.add(Dense(32, activation=activation))\n",
    "dl_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "dl_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0001), metrics=[get_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8764 samples, validate on 2192 samples\n",
      "Epoch 1/10\n",
      "8764/8764 [==============================] - 0s 54us/sample - loss: 0.6668 - get_f1: 0.0125 - val_loss: 0.6288 - val_get_f1: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8764/8764 [==============================] - 0s 35us/sample - loss: 0.5599 - get_f1: 0.1313 - val_loss: 0.4592 - val_get_f1: 0.5275\n",
      "Epoch 3/10\n",
      "8764/8764 [==============================] - 0s 34us/sample - loss: 0.3519 - get_f1: 0.7947 - val_loss: 0.2482 - val_get_f1: 0.9103\n",
      "Epoch 4/10\n",
      "8764/8764 [==============================] - 0s 34us/sample - loss: 0.1865 - get_f1: 0.9424 - val_loss: 0.1373 - val_get_f1: 0.9539\n",
      "Epoch 5/10\n",
      "8764/8764 [==============================] - 0s 32us/sample - loss: 0.1049 - get_f1: 0.9679 - val_loss: 0.0894 - val_get_f1: 0.9569\n",
      "Epoch 6/10\n",
      "8764/8764 [==============================] - 0s 30us/sample - loss: 0.0698 - get_f1: 0.9716 - val_loss: 0.0720 - val_get_f1: 0.9579\n",
      "Epoch 7/10\n",
      "8764/8764 [==============================] - 0s 27us/sample - loss: 0.0556 - get_f1: 0.9745 - val_loss: 0.0673 - val_get_f1: 0.9499\n",
      "Epoch 8/10\n",
      "8764/8764 [==============================] - 0s 29us/sample - loss: 0.0491 - get_f1: 0.9746 - val_loss: 0.0633 - val_get_f1: 0.9632\n",
      "Epoch 9/10\n",
      "8764/8764 [==============================] - 0s 27us/sample - loss: 0.0453 - get_f1: 0.9755 - val_loss: 0.0614 - val_get_f1: 0.9643\n",
      "Epoch 10/10\n",
      "8764/8764 [==============================] - 0s 31us/sample - loss: 0.0434 - get_f1: 0.9773 - val_loss: 0.0609 - val_get_f1: 0.9642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b58b57ef08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_model.fit(X_train, y_train, validation_split=0.2, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score 0.9736111111111111\n"
     ]
    }
   ],
   "source": [
    "preds = dl_model.predict(X_test)\n",
    "print(\"Test F1 Score\", f1_score(y_test, (preds > 0.7).astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy  0.986126323475721\n"
     ]
    }
   ],
   "source": [
    "t, f = 0, 0\n",
    "pred = (preds > 0.7).astype(np.float32)\n",
    "for i in range(len(y_test)):\n",
    "    if pred[i] == y_test[i]:\n",
    "        t+=1\n",
    "    else:\n",
    "        f+=1\n",
    "print(\"Test Accuracy \", t/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.save(\"classifier.h5\")\n",
    "lda.save(\"lda.model\")\n",
    "dictionary.save(\"corpora.dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_k_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
